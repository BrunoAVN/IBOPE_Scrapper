{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Investigando a evolução do IBOPE das emissoras brasileiras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neste trabalho buscamos determinar como evoluiu a audiência das emissoras de televisão aberta nos últimos anos. Para isto utilizaremos os dados disponíveis na página [Kantar Ibope Media](https://www.kantaribopemedia.com/).\n",
    "\n",
    "pseudo-código:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "imports ()\n",
    "for page in 22:\n",
    "    get_urls(page)\n",
    "        | urllib.request.urlopen(page)\n",
    "        | links_list = beautifulSoup(get_links)\n",
    "        * return links_list\n",
    "        \n",
    "for link in link_list:\n",
    "    page = urllib.request.urlopen(link)\n",
    "    scrapper(page)\n",
    "        | for each chanel:\n",
    "        |    get data\n",
    "        | return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-darkgrid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get_URLs:\n",
    "domain = 'https://www.kantaribopemedia.com'\n",
    "\n",
    "link_list = []\n",
    "#complete_data = []\n",
    "\n",
    "def get_link (link):\n",
    "    page = urllib.request.urlopen(link)\n",
    "    soup = BeautifulSoup(page)\n",
    "    article = soup.find_all('article')\n",
    "    links = []\n",
    "    for i in range(len(article)):\n",
    "        path = article[i].h2.a.get('href')\n",
    "        links.append(path)\n",
    "    \n",
    "    return links\n",
    "\n",
    "\n",
    "# Scrapper:\n",
    "def scrapper(link, domain):\n",
    "    complete_link = domain + link\n",
    "    page = urllib.request.urlopen(complete_link)\n",
    "    soup = BeautifulSoup(page)\n",
    "    \n",
    "    day_raw = soup.h1.contents[0][-10:]\n",
    "    \n",
    "    day_raw = test_fmt_data(day_raw, soup)\n",
    "    \n",
    "    day = datetime.strptime(day_raw, '%d/%m/%Y')\n",
    "    print(day)\n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    channels = ['Band', 'Globo', 'Record', 'SBT']\n",
    "    \n",
    "    data, cols = channel_finder(channels, tables, day)\n",
    "    \n",
    "    return data, cols\n",
    "\n",
    "\n",
    "# Get the information from each channel\n",
    "def channel_finder(ch_list, tables, day):\n",
    "    c = 0\n",
    "    data = []\n",
    "    for chn in  ch_list:\n",
    "        if chn == 'Band':\n",
    "            c = 0\n",
    "        elif chn == 'Globo':\n",
    "            c = 1\n",
    "        elif chn == 'Record':\n",
    "            c = 2\n",
    "        elif chn == 'RedeTV':\n",
    "            c = 3\n",
    "        elif chn == 'SBT':\n",
    "            c = 4\n",
    "        else:\n",
    "            raise Exception('ERRO! {} não é um canal válido'.format(chn.upper()))\n",
    "            \n",
    "        try:\n",
    "            find_chn = tables[c].find_all('tr')\n",
    "        except IndexError:\n",
    "            pass\n",
    "\n",
    "        channel_name = str(find_chn[0].find('th').contents[0]).upper()\n",
    "        cols = [str(i.contents[0]) for i in find_chn[2].find_all('td')[0:3]]\n",
    "        cols.insert(0, 'Programa')\n",
    "        cols.insert(0, 'Data')\n",
    "        cols.insert(0, 'Canal')\n",
    "\n",
    "        values = []\n",
    "        programs = []\n",
    "        for j in find_chn[4:]:\n",
    "            try:\n",
    "                name = [str(i).upper() for i in j.find_all('td')[0]]\n",
    "                num = [float(str(i.contents[0]).replace('.', '').replace(',','.')) for i in j.find_all('td')[1:4]]\n",
    "                values.append(num)\n",
    "                programs.append(name)\n",
    "            except IndexError:\n",
    "                name = ['ERROR']\n",
    "                num = [np.nan, np.nan, np.nan]\n",
    "                programs.append(name)\n",
    "                values.append(num)\n",
    "                pass\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "        for i in range(len(values)):\n",
    "            values[i].insert(0, programs[i][0])\n",
    "            values[i].insert(0, day)\n",
    "            values[i].insert(0, channel_name)\n",
    "        \n",
    "        data.extend(values)    \n",
    "    \n",
    "    return data, cols\n",
    "\n",
    "\n",
    "def test_fmt_data(day_raw, soup):\n",
    "    if 'a' in day_raw:\n",
    "        raw = day_raw[-5:]\n",
    "        raw_month = int(raw[3:5])\n",
    "        \n",
    "        post = str(soup.time.contents[0])\n",
    "        post_month = int(post[3:5])\n",
    "        post_year = int(post[-4:])\n",
    "        \n",
    "        if post_month == raw_month:\n",
    "            raw = raw + '/' + str(post_year)\n",
    "            return raw\n",
    "            \n",
    "        elif post_month < raw_month:\n",
    "            year = post_year - 1\n",
    "            raw = raw + '/' + str(year)\n",
    "            return raw\n",
    "        \n",
    "        elif post_month > raw_month:\n",
    "            raw = raw + '/' + str(post_year)\n",
    "            return raw\n",
    "            #raise Exception('ERRO: O mês do post ({}) é maior que o mês de análise({})'.format(post_month, raw_month))\n",
    "    else:\n",
    "        return day_raw\n",
    "    \n",
    "\n",
    "for PAGE in range(12,23):\n",
    "    link  = 'https://www.kantaribopemedia.com/conteudo/dados-rankings/audiencia-tv-15-mercados/page/{}/'.format(PAGE)\n",
    "    list_element = get_link(link)\n",
    "    link_list.append(list_element)\n",
    "    time.sleep(1)\n",
    "\n",
    "\n",
    "count = 0    \n",
    "\n",
    "for line_link in link_list:\n",
    "    \n",
    "    for link in line_link:\n",
    "        part_data, columns = scrapper(link, domain)\n",
    "        complete_data.extend(part_data)\n",
    "    \n",
    "    count += 1\n",
    "    print('Line{}'.format(count))\n",
    "    time.sleep(5)\n",
    "    \n",
    "print('DONE!')\n",
    "#print(complete_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(complete_data, columns=columns)\n",
    "means = df.pivot_table(index = ['Canal','Data'], values=['Audiência Domiciliar','Audiência Individual', 'COV % Individual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means['Audiência Individual']['SBT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "means['Audiência Individual']['GLOBO'][:170].plot(label = 'GLOBO INDV.');\n",
    "means['Audiência Domiciliar']['GLOBO'][:170].plot(label = 'GLOBO DOM.');\n",
    "means['COV % Individual']['GLOBO'][:170].plot(label = 'GLOBO COV');\n",
    "#means['Audiência Domiciliar']['GLOBO'][:170].plot(label = 'GLOBO');\n",
    "\n",
    "#ma_4w.plot(label = 'Globo média móvel')\n",
    "#means['Audiência Individual']['RECORD'][:170].plot( label = 'RECORD');\n",
    "#means['Audiência Individual']['SBT'][:170].plot( label = 'SBT');\n",
    "plt.legend(prop = {'size': 10});\n",
    "\n",
    "#means['Audiência Domiciliar']['GLOBO'].nlargest(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errado_page = urllib.request.urlopen('https://www.kantaribopemedia.com/dados-de-audiencia-nas-15-pracas-regulares-com-base-no-ranking-consolidado-1604-a-2204/')\n",
    "certo_page = urllib.request.urlopen('https://www.kantaribopemedia.com/dados-de-audiencia-nas-15-pracas-regulares-com-base-no-ranking-consolidado-1510-a-2110/')\n",
    "certo_soup = BeautifulSoup(certo_page)\n",
    "errado_soup = BeautifulSoup(errado_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tables = errado_soup.find_all('table')\n",
    "for c in range(0,4):\n",
    "    find_chn = tables[c].find_all('tr')\n",
    "    values = []\n",
    "    programs = []\n",
    "    print(c)\n",
    "    name = 0\n",
    "    for j in find_chn[4:]:\n",
    "        name = [str(i).upper() for i in j.find_all('td')[0]]\n",
    "        print(name)\n",
    "        num = [float(str(i.contents[0]).replace('.', '').replace(',','.')) for i in j.find_all('td')[1:4]]\n",
    "        print(num)\n",
    "        values.append(num)\n",
    "        programs.append(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "certo_soup.find_all('table')[4].find_all('tr')[4:][9].find_all('td')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(errado_soup.find_all('table'))#[4].find_all('tr')[4:][9].find_all('td')[0])\n",
    "\n",
    "### 2018-10-14 00:00:00"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Scrapper:\n",
    "def scrapper(link, domain):\n",
    "    complete_link = domain + link\n",
    "    page = urllib.request.urlopen(complete_link)\n",
    "    soup = BeautifulSoup(page)\n",
    "    \n",
    "    day_raw = soup.h1.contents[0][-10:]\n",
    "    day = datetime.strptime(day_raw, '%d/%m/%Y')\n",
    "    \n",
    "    tables = soup.find_all('table')\n",
    "    \n",
    "    channels = ['Band', 'Globo', 'Record', 'RedeTV', 'SBT']\n",
    "    \n",
    "    data = channel_finder(channels, tables, day)\n",
    "    \n",
    "    return(data)\n",
    "    \n",
    "    \n",
    "def channel_finder(ch_list, tables, day):\n",
    "    c = 0\n",
    "    data = []\n",
    "    for chn in  ch_list:\n",
    "        if chn == 'Band':\n",
    "            c = 0\n",
    "        elif chn == 'Globo':\n",
    "            c = 1\n",
    "        elif chn == 'Record':\n",
    "            c = 2\n",
    "        elif chn == 'RedeTV':\n",
    "            c = 3\n",
    "        elif chn == 'SBT':\n",
    "            c = 4\n",
    "        else:\n",
    "            raise Exception('ERRO! {} não é um canal válido'.format(chn.upper()))\n",
    "            \n",
    "            \n",
    "        find_chn = tables[c].find_all('tr')\n",
    "\n",
    "        channel_name = str(find_chn[0].find('th').contents[0])\n",
    "        cols = [str(i.contents[0]) for i in find_chn[2].find_all('td')[0:3]]\n",
    "        cols.insert(0, 'Programa')\n",
    "        cols.insert(0, 'Data')\n",
    "        cols.insert(0, 'Canal')\n",
    "\n",
    "        values = []\n",
    "        programs = []\n",
    "        for j in find_chn[4:]:\n",
    "            name = [str(i) for i in j.find_all('td')[0]]\n",
    "            num = [float(str(i.contents[0]).replace(',','.')) for i in j.find_all('td')[1:4]]\n",
    "            values.append(num)\n",
    "            programs.append(name)\n",
    "            \n",
    "\n",
    "        for i in range(len(values)):\n",
    "            values[i].insert(0, programs[i][0])\n",
    "            values[i].insert(0, day)\n",
    "            values[i].insert(0, channel_name)\n",
    "        \n",
    "        data.append(values)    \n",
    "    \n",
    "    return(data)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "day = soup.find('h1').contents[0][-10:]\n",
    "day_fmt = datetime.strptime(day, '%d/%m/%Y')\n",
    "\n",
    "find_chn = tables[1].find_all('tr')\n",
    "\n",
    "\n",
    "channel = str(find_chn[0].find('th').contents[0])\n",
    "cols = [str(i.contents[0]) for i in find_chn[2].find_all('td')[0:3]]\n",
    "cols.insert(0, 'Programa')\n",
    "cols.insert(0, 'Data')\n",
    "cols.insert(0, 'Canal')\n",
    "\n",
    "values = []\n",
    "programs = []\n",
    "for j in find_chn[4:]:\n",
    "    name = [str(i) for i in j.find_all('td')[0]]\n",
    "    num = [float(str(i.contents[0]).replace(',','.')) for i in j.find_all('td')[1:4]]\n",
    "    values.append(num)\n",
    "    programs.append(name)\n",
    "    \n",
    "for i in range(len(values)):\n",
    "    values[i].insert(0, programs[i][0])\n",
    "    values[i].insert(0, day_fmt)\n",
    "    values[i].insert(0, channel)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
